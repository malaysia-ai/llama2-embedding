{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('mesolitica/llama2-embedding-1b-8k', \n",
    "                                  trust_remote_code = True, torch_dtype = torch.float16,\n",
    "                                 use_flash_attention_2=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/llama2-embedding-1b-8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘lowyat’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir lowyat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5270749it [00:32, 179947.65it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "6404202it [2:02:49, 135.48it/s] "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('lowyat.jsonl') as fopen:\n",
    "    for no, l in tqdm(enumerate(fopen)):\n",
    "        filename = os.path.join('lowyat', f'{no}.json')\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "        l = json.loads(l)\n",
    "        padded = tokenizer([l], return_tensors = 'pt', max_length = 20000).to('cuda')\n",
    "        v = model.encode(padded).cpu().detach().numpy()[0].tolist()\n",
    "        data = {\n",
    "            'text': l,\n",
    "            'v': v,\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
